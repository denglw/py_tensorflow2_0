{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.2\n",
      "numpy 1.19.0\n",
      "pandas 1.0.5\n",
      "sklearn 0.23.1\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for model in mpl, np, pd, sklearn, keras:\n",
    "    print(model.__name__, model.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2 = keras.layers.Dense(300)\n",
    "layer1 = keras.layers.Dense(100,input_shape=[None,5])\n",
    "layer1(tf.zeros([10, 5]))\n",
    "#layer1.variables #变量\n",
    "# y = w * x + b \n",
    "#layer1.trainable_variables #可训练变量\n",
    "#help(layer1) # 查看方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "# 回归问题\n",
    "# 加利福利亚\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "#加载数据\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 8.32520000e+00,  4.10000000e+01,  6.98412698e+00,\n",
      "         1.02380952e+00,  3.22000000e+02,  2.55555556e+00,\n",
      "         3.78800000e+01, -1.22230000e+02],\n",
      "       [ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
      "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,\n",
      "         3.78600000e+01, -1.22220000e+02],\n",
      "       [ 7.25740000e+00,  5.20000000e+01,  8.28813559e+00,\n",
      "         1.07344633e+00,  4.96000000e+02,  2.80225989e+00,\n",
      "         3.78500000e+01, -1.22240000e+02],\n",
      "       [ 5.64310000e+00,  5.20000000e+01,  5.81735160e+00,\n",
      "         1.07305936e+00,  5.58000000e+02,  2.54794521e+00,\n",
      "         3.78500000e+01, -1.22250000e+02],\n",
      "       [ 3.84620000e+00,  5.20000000e+01,  6.28185328e+00,\n",
      "         1.08108108e+00,  5.65000000e+02,  2.18146718e+00,\n",
      "         3.78500000e+01, -1.22250000e+02]])\n",
      "array([4.526, 3.585, 3.521, 3.413, 3.422])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "#展示数据 查看规律\n",
    "pprint.pprint(housing.data[0:5])\n",
    "pprint.pprint(housing.target[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#数据切割 训练集 验证集 测试\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=7, test_size=0.25)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#数据归一化\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled =  scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# metric\n",
    "metric = keras.metrics.MeanSquaredError() # mse 均方差\n",
    "print(metric([5.0],[2.0])) # (5-2)^2 = 9\n",
    "print(metric([0.0],[1.0])) # (0-1)^2 = 1  -->5  metric累加\n",
    "metric.reset_states()\n",
    "print(metric([0.0],[1.0])) # (0-1)^2 = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动求导，训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 0  train mse: 4.3518745 0  train mse: 3.057971 0  train mse: 2.929299 0  train mse: 2.3832974 0  train mse: 2.2926095 0  train mse: 1.9705106 0  train mse: 1.8750824 0  train mse: 1.8538383 0  train mse: 1.8020703 0  train mse: 1.7865103 0  train mse: 1.7473433 0  train mse: 1.7439405  train mse: 1.707716 0  train mse: 1.6948098 0  train mse: 1.6520303 0  train mse: 1.9621495 0  train mse: 1.9554669 0  train mse: 1.9475605 0  train mse: 1.895981  train mse: 1.891682 0  train mse: 3.0101323 0  train mse: 2.9990494 0  train mse: 2.8763258 0  train mse: 4.966491 0  train mse: 4.8671045 0  train mse: 4.8230886 0  train mse: 4.598618 0  train mse: 4.559496\t valid mse:  1.4890538508259699\n",
      "Epoch 1  train mse: 1.2675031\t1  train mse: 1.2740084 1  train mse: 1.2795768 1  train mse: 1.282601 1  train mse: 1.2694902 1  train mse: 1.27564241  train mse: 1.2618278 1  train mse: 1.2587907 1  train mse: 1.2556528 1  train mse: 1.2704726 valid mse:  1.4022286804899016\n",
      "Epoch 2  train mse: 1.2676556\t2  train mse: 1.2650286 2  train mse: 1.2592567 2  train mse: 1.2576405 2  train mse: 1.259672 2  train mse: 1.25408672  train mse: 1.274582  train mse: 1.2792668 2  train mse: 1.2727063 2  train mse: 1.2680178 2  train mse: 1.2721956 valid mse:  1.3953251281630181\n",
      "Epoch 3  train mse: 1.27075393  train mse: 1.2197127 3  train mse: 1.2596205 3  train mse: 1.2678983 3  train mse: 1.261299 3  train mse: 1.2588755 3  train mse: 1.2595148 3  train mse: 1.2737329 3  train mse: 1.268918 3  train mse: 1.2701079 3  train mse: 1.2657872 3  train mse: 1.2762927 3  train mse: 1.2710127\t valid mse:  1.3925353946222345\n",
      "Epoch 4  train mse: 1.3159307\t 4  train mse: 1.320171 4  train mse: 1.3582122 4  train mse: 1.3529747 4  train mse: 1.3610024 4  train mse: 1.3640307 4  train mse: 1.3420116 4  train mse: 1.3455092 4  train mse: 1.3452858 4  train mse: 1.3368872 4  train mse: 1.3287176 4  train mse: 1.3270801 4  train mse: 1.316208 valid mse:  1.3878610301753695\n",
      "Epoch 5  train mse: 1.2868106 5  train mse: 1.2411827 5  train mse: 1.2394091 train mse: 1.2361938 5  train mse: 1.2610865 train mse: 1.2698807 5  train mse: 1.2794073 5  train mse: 1.2900238 1.289178 5  train mse: 1.2882179 5  train mse: 1.2987332 5  train mse: 1.2933575\t valid mse:  1.388218596161387\n",
      "Epoch 6  train mse: 1.2668245 1.3568342 6  train mse: 1.3024381 6  train mse: 1.3034612 6  train mse: 1.2805434 6  train mse: 1.2752264 6  train mse: 1.2722188 6  train mse: 1.2711998 train mse: 1.2627653 6  train mse: 1.2568439\t valid mse:  1.386248048858877\n",
      "Epoch 7  train mse: 1.2780797\t valid mse:  1.3927645662566128n mse: 1.2750217 7  train mse: 1.2692052 7  train mse: 1.2896204 7  train mse: 1.3086053 7  train mse: 1.3023019 train mse: 1.2977719 7  train mse: 1.295194 7  train mse: 1.2965938 7  train mse: 1.290554 7  train mse: 1.288007 7  train mse: 1.2884562 7  train mse: 1.2839094 7  train mse: 1.2835401\n",
      "Epoch 8  train mse: 1.2704393\t valid mse:  1.3925993377685182n mse: 1.1972792 8  train mse: 1.2254422 8  train mse: 1.2390398 8  train mse: 1.2368093 8  train mse: 1.2464303 8  train mse: 1.2514029  train mse: 1.283112 8  train mse: 1.2804981 8  train mse: 1.2846845 8  train mse: 1.285739 8  train mse: 1.271827\n",
      "Epoch 9  train mse: 1.2565919\t valid mse:  1.386352405196445in mse: 1.2658861.2624604 9  train mse: 1.27073739  train mse: 1.2602668 9  train mse: 1.2542105 9  train mse: 1.2400249 9  train mse: 1.2565635 9  train mse: 1.2599285 9  train mse: 1.2693218 9  train mse: 1.2704307 9  train mse: 1.2675846 9  train mse: 1.2598395\n",
      "Epoch 10  train mse: 1.262107610  train mse: 1.3486769 10  train mse: 1.2416713 10  train mse: 1.26384591.2921472 10  train mse: 1.286602 10  train mse: 1.2917587 10  train mse: 1.2663639 10  train mse: 1.2666236 10  train mse: 1.253469510  train mse: 1.2498527 10  train mse: 1.2556229 10  train mse: 1.2612588\t valid mse:  1.3863367267408158\n",
      "Epoch 11  train mse: 1.2524959\t valid mse:  1.3850617676275099ain mse: 1.3229795 11  train mse: 1.3167393 11  train mse: 1.2973477 11  train mse: 1.2949702 11  train mse: 1.2876348 11  train mse: 1.26793181.2521226 11  train mse: 1.2536589 11  train mse: 1.2465483 train mse: 1.24664\n",
      "Epoch 12  train mse: 1.2626026\t valid mse:  1.3859643555421468ain mse: 1.3741432 1.3541912  train mse: 1.3363456 12  train mse: 1.312457212  train mse: 1.30241 12  train mse: 1.2823545 12  train mse: 1.2825526 12  train mse: 1.2721771 12  train mse: 1.2668684 12  train mse: 1.2586643 12  train mse: 1.2512668\n",
      "Epoch 13  train mse: 1.3060747\t valid mse:  1.385262902770201rain mse: 1.3123646 13  train mse: 1.3531959 1.3330624 13  train mse: 1.3422784 13  train mse: 1.3219496 13  train mse: 1.315722213  train mse: 1.3108048 train mse: 1.2860527 13  train mse: 1.2805018 13  train mse: 1.2794545 13  train mse: 1.2896538 13  train mse: 1.3041099\n",
      "Epoch 14  train mse: 1.2666291 14  train mse: 1.2954779 14  train mse: 1.3139749 14  train mse: 1.27152 14  train mse: 1.2591419 14  train mse: 1.2598321 14  train mse: 1.2658111 14  train mse: 1.2572513 14  train mse: 1.2680566  train mse: 1.2694395 14  train mse: 1.265403\t valid mse:  1.3862784974493465\n",
      "Epoch 15  train mse: 1.268081\t valid mse:  1.386521381998073train mse: 1.2166637 15  train mse: 1.2304716 15  train mse: 1.2532693 15  train mse: 1.242805 15  train mse: 1.243956 15  train mse: 1.2603939 15  train mse: 1.262109 15  train mse: 1.2617327 15  train mse: 1.2712922 15  train mse: 1.2702138 15  train mse: 1.269865\n",
      "Epoch 16  train mse: 1.2774526\t valid mse:  1.387917273346942n mse: 1.337328 16  train mse: 1.3248415 16  train mse: 1.3716948 16  train mse: 1.293075 16  train mse: 1.279731 16  train mse: 1.2795794 16  train mse: 1.2584847 16  train mse: 1.2571856 16  train mse: 1.2716802 16  train mse: 1.277381 16  train mse: 1.2765167 16  train mse: 1.2788931\n",
      "Epoch 17  train mse: 1.2618394 17  train mse: 1.3165971 17  train mse: 1.2901772 17  train mse: 1.2717371 17  train mse: 1.2190883 17  train mse: 1.2341976 17  train mse: 1.2450083 17  train mse: 1.2545182 17  train mse: 1.2396168 17  train mse: 1.2407126 17  train mse: 1.2550491 17  train mse: 1.252281 17  train mse: 1.261115 17  train mse: 1.2627234\t valid mse:  1.3861662288373182\n",
      "Epoch 18  train mse: 1.2328925\t valid mse:  1.3912282786341719ain mse: 1.290413 1.2798197 18  train mse: 1.2733144 18  train mse: 1.2591648 18  train mse: 1.2558559 18  train mse: 1.2454333 18  train mse: 1.2444456 18  train mse: 1.2420684 18  train mse: 1.2383801 18  train mse: 1.235799\n",
      "Epoch 19  train mse: 1.2470719\t valid mse:  1.3846301019395888424 19  train mse: 1.2572947 19  train mse: 1.24849261.2517375 19  train mse: 1.2392701 19  train mse: 1.231338419  train mse: 1.2357781 19  train mse: 1.2348871 19  train mse: 1.2447807 19  train mse: 1.2452186 19  train mse: 1.250477 19  train mse: 1.2470554\n",
      "Epoch 20  train mse: 1.2595123\t valid mse:  1.3863348385317886ain mse: 1.2241876 20  train mse: 1.2673346 20  train mse: 1.2556213 20  train mse: 1.2343957 20  train mse: 1.2507051 20  train mse: 1.2485267 20  train mse: 1.2605852 train mse: 1.2636167 20  train mse: 1.2644123 20  train mse: 1.2545820  train mse: 1.2556481 20  train mse: 1.2637403 20  train mse: 1.263932 20  train mse: 1.2618321\n",
      "Epoch 21  train mse: 1.2705487\t valid mse:  1.384866678867425 mse: 1.2790129 21  train mse: 1.2442226 21  train mse: 1.2382475 21  train mse: 1.2392163 21  train mse: 1.2541729 21  train mse: 1.2658077 1.2708465 21  train mse: 1.2626365 21  train mse: 1.26770921  train mse: 1.2687246 1.2704314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22  train mse: 1.2806416\t valid mse:  1.3865104714274241ain mse: 1.234671 22  train mse: 1.2653755 22  train mse: 1.2620964 22  train mse: 1.2625827 22  train mse: 1.26067141.2702905 22  train mse: 1.2764114 22  train mse: 1.2830596 22  train mse: 1.2814624\n",
      "Epoch 23  train mse: 1.2814798\t valid mse:  1.388579764717286rain mse: 1.258870623  train mse: 1.2489007 23  train mse: 1.2705833 23  train mse: 1.2661839 23  train mse: 1.2566411  train mse: 1.2533942 23  train mse: 1.2632129 23  train mse: 1.2635767 23  train mse: 1.2705275 23  train mse: 1.2806835  train mse: 1.2808486 23  train mse: 1.284785\n",
      "Epoch 24  train mse: 1.2779741 24  train mse: 1.2443523 24  train mse: 1.2517081 24  train mse: 1.233981 24  train mse: 1.2901995 24  train mse: 1.298170124  train mse: 1.2933277 24  train mse: 1.2915903 24  train mse: 1.286803 24  train mse: 1.28175 24  train mse: 1.2809379\t valid mse:  1.393048457165204\n",
      "Epoch 25  train mse: 1.2585578 25  train mse: 1.1904514 25  train mse: 1.2539083 25  train mse: 1.2513326 25  train mse: 1.2632296 1.2710825 25  train mse: 1.2698882 25  train mse: 1.265113 25  train mse: 1.2688028 1.2604427 25  train mse: 1.2618195\t valid mse:  1.3893161398328298\n",
      "Epoch 26  train mse: 1.2400335 26  train mse: 1.1904987 26  train mse: 1.1793323 26  train mse: 1.2022852 26  train mse: 1.2068932 26  train mse: 1.2289236 26  train mse: 1.2350085 26  train mse: 1.2421877 26  train mse: 1.2405313 1.2367424 26  train mse: 1.2365248 26  train mse: 1.2360593\t valid mse:  1.3882271490334097\n",
      "Epoch 27  train mse: 1.2884185\t valid mse:  1.3861541188529003ain mse: 1.2960742 27  train mse: 1.3062302 27  train mse: 1.2958783 27  train mse: 1.2954521 27  train mse: 1.2962282 27  train mse: 1.2791704 27  train mse: 1.2894987 27  train mse: 1.2990873 27  train mse: 1.2829368 27  train mse: 1.2789413 27  train mse: 1.2906975\n",
      "Epoch 28  train mse: 1.2404023\t valid mse:  1.384097543041337train mse: 1.3575816 28  train mse: 1.2246947 28  train mse: 1.2309766  train mse: 1.2396152 28  train mse: 1.2446144 28  train mse: 1.23246471.2315521 28  train mse: 1.2326717 28  train mse: 1.2386501 28  train mse: 1.231162 28  train mse: 1.2310936 28  train mse: 1.2428721 28  train mse: 1.2360101\n",
      "Epoch 29  train mse: 1.2732884\t valid mse:  1.387942620871691.1985799 29  train mse: 1.2066996 29  train mse: 1.216432 29  train mse: 1.2105747 29  train mse: 1.2294445 29  train mse: 1.228061 29  train mse: 1.2255372 29  train mse: 1.2432668 29  train mse: 1.2462429 29  train mse: 1.2562343 29  train mse: 1.2723652\n",
      "Epoch 30  train mse: 1.2443225 30  train mse: 1.439832430  train mse: 1.2978272 train mse: 1.266107  train mse: 1.29141 30  train mse: 1.2450072 30  train mse: 1.2357949 30  train mse: 1.2395827 30  train mse: 1.2350694 30  train mse: 1.2452844 30  train mse: 1.2441623 30  train mse: 1.2417156 30  train mse: 1.2387846 30  train mse: 1.244004\t valid mse:  1.388966671818227\n",
      "Epoch 31  train mse: 1.2676677\t valid mse:  1.3844977076778515ain mse: 1.2409127 1.2586912 31  train mse: 1.2765253 31  train mse: 1.268888 31  train mse: 1.271599 31  train mse: 1.2668774 31  train mse: 1.2659723 31  train mse: 1.271177331  train mse: 1.2712569\n",
      "Epoch 32  train mse: 1.2929717\t valid mse:  1.3835570131912556ain mse: 1.2593957 32  train mse: 1.2544204 32  train mse: 1.2570279 32  train mse: 1.2669275 32  train mse: 1.2894452 32  train mse: 1.2971694 32  train mse: 1.293991432  train mse: 1.2876914 1.291131532  train mse: 1.3032953\n",
      "Epoch 33  train mse: 1.2433218\t valid mse:  1.3932539012864553in mse: 1.2586892 33  train mse: 1.2661095 33  train mse: 1.2688476 33  train mse: 1.2578782 33  train mse: 1.2458804 33  train mse: 1.246312 33  train mse: 1.2421653 33  train mse: 1.2505782 33  train mse: 1.2462621 33  train mse: 1.2475626 33  train mse: 1.2452719 33  train mse: 1.2501706 33  train mse: 1.2479573\n",
      "Epoch 34  train mse: 1.2450875\t valid mse:  1.386586831620097train mse: 1.1839823 34  train mse: 1.2370659 34  train mse: 1.2393479 34  train mse: 1.2279173 34  train mse: 1.2330517 34  train mse: 1.2277497 34  train mse: 1.2350017 34  train mse: 1.2361351 34  train mse: 1.2448323\n",
      "Epoch 35  train mse: 1.25112576 35  train mse: 1.1313106 35  train mse: 1.251328335  train mse: 1.2570796 35  train mse: 1.2509601 35  train mse: 1.2456528 35  train mse: 1.2321913 35  train mse: 1.2453882 35  train mse: 1.243899 35  train mse: 1.2412039 35  train mse: 1.2539251 35  train mse: 1.2417055 35  train mse: 1.2375432 35  train mse: 1.2415135\t valid mse:  1.3846428370449817\n",
      "Epoch 36  train mse: 1.2626781\t valid mse:  1.3888028949100921rain mse: 1.3091415 36  train mse: 1.2916086 36  train mse: 1.28602651.2884499 36  train mse: 1.2811317 1.280628 36  train mse: 1.2758002 36  train mse: 1.2692612 36  train mse: 1.2621235 36  train mse: 1.2620296\n",
      "Epoch 37  train mse: 1.3183868\t valid mse:  1.3838630473018327mse: 1.3029151 37  train mse: 1.2960746 37  train mse: 1.2795105 37  train mse: 1.2909942 37  train mse: 1.2824781 37  train mse: 1.2929153 37  train mse: 1.3138754 37  train mse: 1.3123137 37  train mse: 1.3150586 37  train mse: 1.3083713 37  train mse: 1.3159208 37  train mse: 1.3184642 37  train mse: 1.3107806 37  train mse: 1.3168666\n",
      "Epoch 38  train mse: 1.2516987\t38  train mse: 1.352186 1.2948264 38  train mse: 1.24357931.2364498 38  train mse: 1.2465599 38  train mse: 1.2354394 38  train mse: 1.2368474 38  train mse: 1.2440846 38  train mse: 1.2459861 38  train mse: 1.2490032 38  train mse: 1.2513616 valid mse:  1.3874143163032404\n",
      "Epoch 39  train mse: 1.26019855 39  train mse: 1.3358356 39  train mse: 1.2631841 39  train mse: 1.2318567 39  train mse: 1.2207146 39  train mse: 1.2301123 train mse: 1.2329274 39  train mse: 1.2480373 39  train mse: 1.2691579 39  train mse: 1.2712636 39  train mse: 1.2593756 39  train mse: 1.2532268 train mse: 1.2552623 39  train mse: 1.2565554 39  train mse: 1.2641169\t valid mse:  1.3833828366660936\n",
      "Epoch 40  train mse: 1.24714616 40  train mse: 1.1771995 40  train mse: 1.2060547 40  train mse: 1.2202206  train mse: 1.2372444 40  train mse: 1.2405868 40  train mse: 1.22547 40  train mse: 1.2241696 1.2308121 train mse: 1.2345169 40  train mse: 1.2447689 40  train mse: 1.2446359 40  train mse: 1.2450835\t valid mse:  1.3853680947292288\n",
      "Epoch 41  train mse: 1.2584414\t valid mse:  1.3845028210680546in mse: 1.2162908 41  train mse: 1.2635868 41  train mse: 1.2554488 41  train mse: 1.2350726 1.2382593 41  train mse: 1.2583963 41  train mse: 1.2521704 41  train mse: 1.2467539 41  train mse: 1.2538912 41  train mse: 1.2574607\n",
      "Epoch 42  train mse: 1.2455657\t valid mse:  1.3839981353248527rain mse: 1.2346436 42  train mse: 1.2482175 42  train mse: 1.2763792 42  train mse: 1.2501856 42  train mse: 1.2525029 42  train mse: 1.2536929 42  train mse: 1.2516656 42  train mse: 1.2461607 train mse: 1.2444888 42  train mse: 1.2439841\n",
      "Epoch 43  train mse: 1.2923578 43  train mse: 1.2984595 43  train mse: 1.3933436 1.3872994 43  train mse: 1.354983 43  train mse: 1.3565152 43  train mse: 1.307557643  train mse: 1.2894272 43  train mse: 1.3128474 43  train mse: 1.3087264 43  train mse: 1.2986203 43  train mse: 1.2893862 43  train mse: 1.295397\t valid mse:  1.3839526049763267\n",
      "Epoch 44  train mse: 1.2684431 44  train mse: 1.2396843 1.2108468 44  train mse: 1.2194656 44  train mse: 1.2170702 44  train mse: 1.2269905  train mse: 1.2440568 44  train mse: 1.2395216 44  train mse: 1.2352926 44  train mse: 1.2336066 44  train mse: 1.241603 44  train mse: 1.2621129\t valid mse:  1.383169535267625\n",
      "Epoch 45  train mse: 1.2715015\t45  train mse: 1.3246957 45  train mse: 1.3417088 45  train mse: 1.3349091 45  train mse: 1.2915761 45  train mse: 1.2696058 45  train mse: 1.2594304 45  train mse: 1.2587044 45  train mse: 1.2658608 45  train mse: 1.2745781 45  train mse: 1.27441 45  train mse: 1.2772424 45  train mse: 1.2803181 45  train mse: 1.2728981 valid mse:  1.384285812001638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46  train mse: 1.26633\t valid mse:  1.3912623696032638 train mse: 1.2768569 46  train mse: 1.2735518  train mse: 1.2662606 46  train mse: 1.2743638 46  train mse: 1.2811111 46  train mse: 1.2748806 46  train mse: 1.2694597 46  train mse: 1.2797897 46  train mse: 1.2844427 46  train mse: 1.2796092 46  train mse: 1.2789818\n",
      "Epoch 47  train mse: 1.2620338\t valid mse:  1.3838329686076327921 47  train mse: 1.2509809 47  train mse: 1.2391349 47  train mse: 1.226333 47  train mse: 1.2406689 47  train mse: 1.2251259 47  train mse: 1.231163 47  train mse: 1.236189 train mse: 1.2529346 47  train mse: 1.2583644 47  train mse: 1.2603532\n",
      "Epoch 48  train mse: 1.2463421\t 48  train mse: 1.2679604 48  train mse: 1.2991143 48  train mse: 1.285708748  train mse: 1.2606721 48  train mse: 1.2479789 48  train mse: 1.2481421 48  train mse: 1.2430577 48  train mse: 1.2504032 48  train mse: 1.247921 48  train mse: 1.2452836 valid mse:  1.3870904456764321\n",
      "Epoch 49  train mse: 1.243665\t valid mse:  1.3865212131690732train mse: 1.1917744 49  train mse: 1.2163589 49  train mse: 1.202544149  train mse: 1.2131093 49  train mse: 1.2235587 49  train mse: 1.2282648 49  train mse: 1.2365113 49  train mse: 1.247918 49  train mse: 1.2439482 49  train mse: 1.2489027 49  train mse: 1.246412\n",
      "Epoch 50  train mse: 1.2831794\t50  train mse: 1.5489284 50  train mse: 1.3637176 1.3221225  train mse: 1.3240839 50  train mse: 1.3066776 train mse: 1.299136650  train mse: 1.2789208 50  train mse: 1.279428 50  train mse: 1.273548850  train mse: 1.2768801 valid mse:  1.3838411345904487\n",
      "Epoch 51  train mse: 1.2623458 1.1202683 51  train mse: 1.198792 51  train mse: 1.2169646 51  train mse: 1.222466151  train mse: 1.2414101 train mse: 1.248325151  train mse: 1.2549673 1.2605041  train mse: 1.262523\t valid mse:  1.3829381403730157\n",
      "Epoch 52  train mse: 1.2885888 52  train mse: 1.2806265 52  train mse: 1.3055508 52  train mse: 1.3165182 52  train mse: 1.3090482 52  train mse: 1.2898875 52  train mse: 1.2836984 52  train mse: 1.277867 52  train mse: 1.279224 52  train mse: 1.2827293 52  train mse: 1.2897632 52  train mse: 1.2912222\t valid mse:  1.3840735374183009\n",
      "Epoch 53  train mse: 1.2619252\t valid mse:  1.38519784728759train mse: 1.2853445 53  train mse: 1.218598 1.214762253  train mse: 1.219755 53  train mse: 1.2500079 53  train mse: 1.2596323 53  train mse: 1.2675502 53  train mse: 1.2637756 1.2654123 53  train mse: 1.2674168 53  train mse: 1.269439253  train mse: 1.2650437\n",
      "Epoch 54  train mse: 1.2686164\t valid mse:  1.3855011840852856ain mse: 1.2417941 54  train mse: 1.257019954  train mse: 1.2632502 54  train mse: 1.2614385 54  train mse: 1.272508 54  train mse: 1.282685 54  train mse: 1.2798506 54  train mse: 1.27457754  train mse: 1.2616376 54  train mse: 1.2589797 54  train mse: 1.2686964\n",
      "Epoch 55  train mse: 1.2536976\t55  train mse: 1.1571213 55  train mse: 1.2072413 55  train mse: 1.238593 55  train mse: 1.243038 55  train mse: 1.2340842 55  train mse: 1.2412903 55  train mse: 1.246515 55  train mse: 1.2548478 55  train mse: 1.2603421 55  train mse: 1.2636445 55  train mse: 1.2583641 55  train mse: 1.248415 valid mse:  1.386620919673121\n",
      "Epoch 56  train mse: 1.2602386 56  train mse: 1.3212671 56  train mse: 1.249392656  train mse: 1.2465934 56  train mse: 1.2522929 56  train mse: 1.2620405 56  train mse: 1.2803409 56  train mse: 1.2696897 56  train mse: 1.2804776 56  train mse: 1.2696623 56  train mse: 1.2606182 56  train mse: 1.2589209 56  train mse: 1.2612973\t valid mse:  1.384172925434973\n",
      "Epoch 57  train mse: 1.2531576\t valid mse:  1.3834502873010754rain mse: 1.2385871 train mse: 1.2552966 57  train mse: 1.2622527 57  train mse: 1.2421707 57  train mse: 1.2544124 57  train mse: 1.2543411 57  train mse: 1.255265 57  train mse: 1.245171 57  train mse: 1.238851 57  train mse: 1.2495246 57  train mse: 1.249168\n",
      "Epoch 58  train mse: 1.2724503\t valid mse:  1.385190041227989rain mse: 1.261144  train mse: 1.2149793 58  train mse: 1.221212 58  train mse: 1.22316971.2363293 train mse: 1.2465267 58  train mse: 1.2509786 58  train mse: 1.2672397\n",
      "Epoch 59  train mse: 1.2626281\t59  train mse: 1.2006677 59  train mse: 1.2261698 59  train mse: 1.2584488 59  train mse: 1.241193 59  train mse: 1.2586595 59  train mse: 1.248807359  train mse: 1.2477791 59  train mse: 1.2541009 59  train mse: 1.2474754 59  train mse: 1.2520466 59  train mse: 1.2449458 59  train mse: 1.2502606 59  train mse: 1.2611722 valid mse:  1.3834288024760992\n",
      "Epoch 60  train mse: 1.2804294\t valid mse:  1.3832651999107166in mse: 1.3250208 60  train mse: 1.3230518 60  train mse: 1.3100982 60  train mse: 1.2832822 60  train mse: 1.2554272 60  train mse: 1.284597660  train mse: 1.280895 60  train mse: 1.279415 60  train mse: 1.2720969 60  train mse: 1.2725989 60  train mse: 1.27122 60  train mse: 1.282061\n",
      "Epoch 61  train mse: 1.2553834 61  train mse: 1.2961396 61  train mse: 1.2801247 61  train mse: 1.2476221 61  train mse: 1.2840991 61  train mse: 1.2556956 61  train mse: 1.2362332 61  train mse: 1.2347312 61  train mse: 1.2479804 61  train mse: 1.2558222 61  train mse: 1.2576271 61  train mse: 1.2639122\t valid mse:  1.392799294099225\n",
      "Epoch 62  train mse: 1.235906\t valid mse:  1.384214829427027rain mse: 1.239847 62  train mse: 1.2338085 62  train mse: 1.2382534 62  train mse: 1.2451911 62  train mse: 1.2493447 62  train mse: 1.2542274 62  train mse: 1.2536774  train mse: 1.2440904 62  train mse: 1.243876 62  train mse: 1.2429166 62  train mse: 1.2326496\n",
      "Epoch 63  train mse: 1.2490971\t valid mse:  1.382884692471795train mse: 1.2396259 63  train mse: 1.255875 63  train mse: 1.2372525 63  train mse: 1.2421513 1.2311579 63  train mse: 1.2404945 63  train mse: 1.25786 63  train mse: 1.2588748 63  train mse: 1.2553663 63  train mse: 1.2553198 63  train mse: 1.254158 63  train mse: 1.2475467 63  train mse: 1.2443929 63  train mse: 1.2489302\n",
      "Epoch 64  train mse: 1.2374128\t valid mse:  1.3862031069008467ain mse: 1.154575 train mse: 1.229055 64  train mse: 1.2277588 64  train mse: 1.2372608 64  train mse: 1.2296652 64  train mse: 1.2183632 64  train mse: 1.2284535 64  train mse: 1.2209833 64  train mse: 1.2209907 64  train mse: 1.229344 1.2339216 64  train mse: 1.2347176\n",
      "Epoch 65  train mse: 1.2442677  train mse: 1.1885889 65  train mse: 1.2178115 65  train mse: 1.2260007 train mse: 1.2213937 65  train mse: 1.2180289 65  train mse: 1.2199355 65  train mse: 1.2247669 65  train mse: 1.2254887 1.2236433 65  train mse: 1.2295259 65  train mse: 1.234968\t valid mse:  1.3834363619785401\n",
      "Epoch 66  train mse: 1.2828175\t valid mse:  1.3843537189342243mse: 1.3314415 66  train mse: 1.3312734 66  train mse: 1.339306 66  train mse: 1.3251793 66  train mse: 1.3187693 66  train mse: 1.307443466  train mse: 1.302762366  train mse: 1.298509 66  train mse: 1.2859265\n",
      "Epoch 67  train mse: 1.2600198 67  train mse: 1.2808719 67  train mse: 1.2525263 67  train mse: 1.2534932 67  train mse: 1.2309017 67  train mse: 1.2457701 67  train mse: 1.253503 train mse: 1.2611082 67  train mse: 1.2684075 67  train mse: 1.2580657 67  train mse: 1.2617424 67  train mse: 1.2544761 67  train mse: 1.2588711 67  train mse: 1.2657988 1.2629256\t valid mse:  1.3874872504104914\n",
      "Epoch 68  train mse: 1.2807906\t valid mse:  1.3964838738043213ain mse: 1.188207 68  train mse: 1.2094333 68  train mse: 1.2267994 68  train mse: 1.2448472 68  train mse: 1.244594 68  train mse: 1.2406168 68  train mse: 1.2456774 68  train mse: 1.2559092 68  train mse: 1.2642379 68  train mse: 1.2622068 68  train mse: 1.2784225 68  train mse: 1.2763015 68  train mse: 1.2836349\n",
      "Epoch 69  train mse: 1.2319674\t valid mse:  1.3843984261012359ain mse: 1.2321256 69  train mse: 1.2221004 69  train mse: 1.1967826 69  train mse: 1.2159904 69  train mse: 1.2179643 69  train mse: 1.2047825  train mse: 1.20264 69  train mse: 1.2107999 69  train mse: 1.2267218 69  train mse: 1.2270311 69  train mse: 1.22874 69  train mse: 1.223867469  train mse: 1.231283\n",
      "Epoch 70  train mse: 1.2828832\t valid mse:  1.3826928113608599ain mse: 1.3188006 70  train mse: 1.2957256 70  train mse: 1.3018101 70  train mse: 1.2932959 70  train mse: 1.2916409 70  train mse: 1.2791085 1.27575341.278740570  train mse: 1.279684 70  train mse: 1.281967 70  train mse: 1.2780538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71  train mse: 1.2813854\t valid mse:  1.38375667950105ain mse: 1.3099415 1.293999271  train mse: 1.2901461 71  train mse: 1.2865598 71  train mse: 1.2817086 71  train mse: 1.2740401 71  train mse: 1.2766379 71  train mse: 1.2777604  train mse: 1.2805115\n",
      "Epoch 72  train mse: 1.2258749 72  train mse: 1.2411147 72  train mse: 1.262975 72  train mse: 1.2388326 72  train mse: 1.2468107 72  train mse: 1.2613264 72  train mse: 1.2568198 72  train mse: 1.2430546 72  train mse: 1.2364492 train mse: 1.2317692 72  train mse: 1.2288182\t valid mse:  1.3868295113932385\n",
      "Epoch 73  train mse: 1.2650487\t valid mse:  1.3880886483745947ain mse: 1.2456412 73  train mse: 1.2245443 73  train mse: 1.2808233 73  train mse: 1.2994142 73  train mse: 1.27532 train mse: 1.2744524 73  train mse: 1.2788923 73  train mse: 1.2742771 73  train mse: 1.2746347 73  train mse: 1.2679032 73  train mse: 1.270397 73  train mse: 1.2641463\n",
      "Epoch 74  train mse: 1.2662122 1.28267 74  train mse: 1.287411 74  train mse: 1.3002051 74  train mse: 1.2437345 74  train mse: 1.229543 74  train mse: 1.238498 74  train mse: 1.2485226 74  train mse: 1.2534542 1.2608601\t valid mse:  1.3858640707761944\n",
      "Epoch 75  train mse: 1.24711054 75  train mse: 1.2265921 75  train mse: 1.1670175  train mse: 1.1991837 75  train mse: 1.2248181 75  train mse: 1.2213022 75  train mse: 1.2126724 75  train mse: 1.221773 75  train mse: 1.2286551 75  train mse: 1.2323886 75  train mse: 1.244833175  train mse: 1.23908 75  train mse: 1.2446679 75  train mse: 1.25077\t valid mse:  1.3848229262926113\n",
      "Epoch 76  train mse: 1.2803555\t valid mse:  1.38350831113575916 76  train mse: 1.2202332 76  train mse: 1.2324954 1.2485331 76  train mse: 1.2633383 1.2658985 76  train mse: 1.2667252 76  train mse: 1.2680646 76  train mse: 1.2767557 76  train mse: 1.2760761 76  train mse: 1.280895\n",
      "Epoch 77  train mse: 1.2851869\t valid mse:  1.387992299822995rain mse: 1.373851.343151 77  train mse: 1.3166246 77  train mse: 1.3058815 77  train mse: 1.2844073 77  train mse: 1.2962315 77  train mse: 1.2979746 77  train mse: 1.2850511 77  train mse: 1.2900931 77  train mse: 1.2848235 77  train mse: 1.2855524\n",
      "Epoch 78  train mse: 1.2601415 78  train mse: 1.2813154 78  train mse: 1.2881935 78  train mse: 1.2849979 78  train mse: 1.273057 78  train mse: 1.2722936  train mse: 1.2805618 78  train mse: 1.2713791 78  train mse: 1.2639796 78  train mse: 1.2568755\t valid mse:  1.3873387056692046\n",
      "Epoch 79  train mse: 1.2724385\t valid mse:  1.3845364733845607ain mse: 1.30276381.2666335 79  train mse: 1.2765222 79  train mse: 1.2853502 79  train mse: 1.2834398 79  train mse: 1.288994 train mse: 1.2850078 79  train mse: 1.2800038 79  train mse: 1.2737311.2708257\n",
      "Epoch 80  train mse: 1.238412\t valid mse:  1.3864875482169945rain mse: 1.215928 80  train mse: 1.2043368 80  train mse: 1.238717 80  train mse: 1.2410923 80  train mse: 1.2545382 80  train mse: 1.2519493 80  train mse: 1.247367 80  train mse: 1.2446688 80  train mse: 1.2429237 80  train mse: 1.2453704 80  train mse: 1.2422233 80  train mse: 1.2363557\n",
      "Epoch 81  train mse: 1.2404602\t valid mse:  1.3842042545723756rain mse: 1.265293 81  train mse: 1.2744123 81  train mse: 1.2471164 81  train mse: 1.2644889 81  train mse: 1.2611827 81  train mse: 1.2501872 81  train mse: 1.2554641 81  train mse: 1.253904 81  train mse: 1.2424849\n",
      "Epoch 82  train mse: 1.2572948\t valid mse:  1.387243983643875rain mse: 1.2269673 82  train mse: 1.26264431.2585968 82  train mse: 1.25391 82  train mse: 1.2512662 82  train mse: 1.2357788 1.2347888 82  train mse: 1.2427682 82  train mse: 1.2539597 82  train mse: 1.2501903 82  train mse: 1.2538874 82  train mse: 1.2602016\n",
      "Epoch 83  train mse: 1.2457188 83  train mse: 1.2320127 83  train mse: 1.2006049 83  train mse: 1.22162891.2355344 83  train mse: 1.2606006 83  train mse: 1.2478338 83  train mse: 1.2404296 83  train mse: 1.2343292 83  train mse: 1.2390811 83  train mse: 1.2432232 83  train mse: 1.2394696 83  train mse: 1.2410027 83  train mse: 1.2440939 83  train mse: 1.249309 83  train mse: 1.2452623\t valid mse:  1.3898529080136341\n",
      "Epoch 84  train mse: 1.2366107\t valid mse:  1.384479925467466rain mse: 1.2049468  train mse: 1.1905788 84  train mse: 1.1877493 84  train mse: 1.22192121.215438784  train mse: 1.214526 84  train mse: 1.212428 train mse: 1.2142637 84  train mse: 1.2275028 train mse: 1.2278563\n",
      "Epoch 85  train mse: 1.2940049\t valid mse:  1.3857613066609982ain mse: 1.413417985  train mse: 1.3188794 85  train mse: 1.2884488 85  train mse: 1.2880975 85  train mse: 1.2801782 85  train mse: 1.288789 85  train mse: 1.2873585 85  train mse: 1.3045427 85  train mse: 1.3005278 85  train mse: 1.2976002 train mse: 1.2953944 85  train mse: 1.2976748\n",
      "Epoch 86  train mse: 1.267003\t valid mse:  1.3843016617067536n mse: 1.2987434 86  train mse: 1.2968802 86  train mse: 1.2714512 86  train mse: 1.2517842 86  train mse: 1.2544054 86  train mse: 1.2462163 86  train mse: 1.2493478  train mse: 1.2628814 86  train mse: 1.2651123 86  train mse: 1.2567016 86  train mse: 1.2600256 86  train mse: 1.2652186\n",
      "Epoch 87  train mse: 1.2305297 87  train mse: 1.1816856 87  train mse: 1.182857 87  train mse: 1.204882 87  train mse: 1.2029378 train mse: 1.214331 87  train mse: 1.210846 87  train mse: 1.214917187  train mse: 1.2324654 87  train mse: 1.2328806 87  train mse: 1.234502 87  train mse: 1.2334375 87  train mse: 1.227088\t valid mse:  1.3850258315636244\n",
      "Epoch 88  train mse: 1.2525604\t88  train mse: 1.2533226 88  train mse: 1.2695118 88  train mse: 1.2779255 88  train mse: 1.2792581 88  train mse: 1.259951 88  train mse: 1.254529 88  train mse: 1.2481132 88  train mse: 1.245432 88  train mse: 1.2474918 88  train mse: 1.2505786 88  train mse: 1.2523952 valid mse:  1.383318562341911\n",
      "Epoch 89  train mse: 1.2489254 89  train mse: 1.1968331 89  train mse: 1.2443885 89  train mse: 1.2628543 89  train mse: 1.2527043 89  train mse: 1.2723366 89  train mse: 1.27520689  train mse: 1.2710987  train mse: 1.2647991 89  train mse: 1.2580673 89  train mse: 1.257895789  train mse: 1.260015 89  train mse: 1.2564772\t valid mse:  1.3892621354393153\n",
      "Epoch 90  train mse: 1.2484492\t valid mse:  1.3830408754875445rain mse: 1.2085389  train mse: 1.2173218 90  train mse: 1.2148699 90  train mse: 1.2092566 train mse: 1.2139937 90  train mse: 1.2076634 90  train mse: 1.2234796 90  train mse: 1.222620790  train mse: 1.2281181 90  train mse: 1.2278514 90  train mse: 1.2339232 90  train mse: 1.2425542\n",
      "Epoch 91  train mse: 1.2685692\t valid mse:  1.3853168122938795ain mse: 1.3008746 91  train mse: 1.2857097 91  train mse: 1.2665918 91  train mse: 1.2641742 train mse: 1.275869 91  train mse: 1.2824984 91  train mse: 1.2935042 91  train mse: 1.2913873 91  train mse: 1.2851619 91  train mse: 1.276035991  train mse: 1.2694422\n",
      "Epoch 92  train mse: 1.2576232\t valid mse:  1.3832141749900326ain mse: 1.2208428 train mse: 1.216709 92  train mse: 1.2360473 92  train mse: 1.2393683 92  train mse: 1.2554189 92  train mse: 1.2559648 92  train mse: 1.2722446 92  train mse: 1.2651814 92  train mse: 1.2628183 92  train mse: 1.253771 92  train mse: 1.2528529 92  train mse: 1.2539384\n",
      "Epoch 93  train mse: 1.2741963 93  train mse: 1.4088411 93  train mse: 1.2848191 93  train mse: 1.2701494 93  train mse: 1.2651498 93  train mse: 1.2625396 93  train mse: 1.2922916 93  train mse: 1.2943516 93  train mse: 1.2891545 93  train mse: 1.2817882 93  train mse: 1.26740491.2637345 93  train mse: 1.2666491 93  train mse: 1.2700338 93  train mse: 1.2749497\t valid mse:  1.3830323130196496\n",
      "Epoch 94  train mse: 1.22039974 94  train mse: 1.0852652 94  train mse: 1.0987949 94  train mse: 1.1479585 94  train mse: 1.1528624 94  train mse: 1.1632189 94  train mse: 1.1898422 94  train mse: 1.2008885 94  train mse: 1.2170393 94  train mse: 1.2061629 94  train mse: 1.2187377 94  train mse: 1.2139149 94  train mse: 1.2201885\t valid mse:  1.3828430306301605\n",
      "Epoch 95  train mse: 1.2466806\t valid mse:  1.3848801051035189ain mse: 1.2297595 95  train mse: 1.2773077 95  train mse: 1.2374861 95  train mse: 1.2379992 95  train mse: 1.2499094 95  train mse: 1.251916 95  train mse: 1.250104895  train mse: 1.2525501 95  train mse: 1.2328968 95  train mse: 1.23896251.2450747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96  train mse: 1.2705266 96  train mse: 1.2762046 96  train mse: 1.2419419 96  train mse: 1.2575446 96  train mse: 1.2352809 96  train mse: 1.2516092 96  train mse: 1.249722696  train mse: 1.2384487 96  train mse: 1.2494309 96  train mse: 1.2523571 96  train mse: 1.2580422\t valid mse:  1.3839705323502736\n",
      "Epoch 97  train mse: 1.2436273\t valid mse:  1.3883244027909934874 97  train mse: 1.2582383 97  train mse: 1.2605215 97  train mse: 1.2598627 97  train mse: 1.258358 97  train mse: 1.2447475 97  train mse: 1.2437235 97  train mse: 1.246406 97  train mse: 1.2557706\n",
      "Epoch 98  train mse: 1.2519162\t valid mse:  1.3855057462848186ain mse: 1.26686741.29046121.2517986 98  train mse: 1.2612975 98  train mse: 1.2631447 98  train mse: 1.2612033 98  train mse: 1.2550496 98  train mse: 1.2609777 98  train mse: 1.2649916 98  train mse: 1.2580581 98  train mse: 1.2554877\n",
      "Epoch 99  train mse: 1.2715746\t valid mse:  1.385293436338815 mse: 1.3102059 99  train mse: 1.3154787 99  train mse: 1.3009453 99  train mse: 1.2985122 99  train mse: 1.303488 99  train mse: 1.2977417 99  train mse: 1.2912247 99  train mse: 1.2962627 99  train mse: 1.3031929 99  train mse: 1.2967556 1.2838538 99  train mse: 1.2748812\n"
     ]
    }
   ],
   "source": [
    "# 1. batch 遍历训练集 metric\n",
    "#    1.1 自动求导\n",
    "# 2. epoch结束 验证集 metric\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(x_train_scaled) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "\n",
    "def random_batch(x, y, batch_size =32):\n",
    "    idx = np.random.randint(0, len(x), size = batch_size)\n",
    "    return x[idx],y[idx]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()\n",
    "    for step in range(steps_per_epoch):\n",
    "        x_batch,y_batch = random_batch(x_train_scaled,y_train,batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = tf.reduce_mean(keras.losses.mean_squared_error(y_batch,y_pred))\n",
    "            metric(y_batch,y_pred)\n",
    "        grads = tape.gradient(loss,model.variables)\n",
    "        grads_and_vars = zip(grads,model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars) # apply_gradients\n",
    "        print(\"\\rEpoch\", epoch, \" train mse:\",metric.result().numpy(), end=\"\")\n",
    "    y_valid_pred = model(x_valid_scaled)\n",
    "    valid_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_valid_pred, y_valid))\n",
    "    print(\"\\t\", \"valid mse: \", valid_loss.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
