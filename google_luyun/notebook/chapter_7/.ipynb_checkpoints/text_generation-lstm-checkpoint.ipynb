{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.2\n",
      "numpy 1.19.0\n",
      "pandas 1.0.5\n",
      "sklearn 0.23.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl,np,pd,sklearn,tf,keras:\n",
    "    print(module.__name__,module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887658\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# text generation\n",
    "# 莎士比亚 剧本生成\n",
    "# https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "input_filepath = \"./shakespeare.txt\"\n",
    "text = open(input_filepath,'r').read()\n",
    "print(len(text))\n",
    "print(text[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# 1. generate vocab\n",
    "# 2. build mapping char->id\n",
    "# 3. data -> id_data\n",
    "# 4. abcd -> bcd<eos>\n",
    "# 生成词表\n",
    "vocab = sorted(set(text))\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# 生成map 字符：索引\n",
    "char2idx = { char:idx for idx,char in enumerate(vocab) }\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# id到char的映射\n",
    "# data -> id_data\n",
    "idx2char = np.array(vocab)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47]\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "# 把text 转成 词表index\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[0:10])\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32) F\n",
      "tf.Tensor(47, shape=(), dtype=int32) i\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(id_text):\n",
    "    \"\"\"\n",
    "    abcde -> abcd, bcde\n",
    "    \"\"\"\n",
    "    return id_text[0:-1],id_text[1:]\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # 字符dataset\n",
    "seq_length = 100\n",
    "# why seq_length + 1 because split_input_target函数\n",
    "seq_dataset = char_dataset.batch(seq_length + 1, drop_remainder =True) # 句子dataset\n",
    "\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id,idx2char[ch_id.numpy()])\n",
    "    \n",
    "\n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "    print(repr(''.join(idx2char[seq_id.numpy()]))) # 字符拼接城句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1]\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49]\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target) # map\n",
    "\n",
    "for item_input,item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "# batch数据集\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (64, None, 1024)          1311744   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 1,395,009\n",
      "Trainable params: 1,395,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# return_sequences=True   why? 每一步输出的序列，最后都需要使用\n",
    "def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
    "        keras.layers.SimpleRNN(units=rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),# stateful, recurrent_initializer\n",
    "        keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "# see模型输出，把model当做函数调用\n",
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[25]\n",
      " [24]\n",
      " [16]\n",
      " [18]\n",
      " [32]\n",
      " [ 8]\n",
      " [40]\n",
      " [51]\n",
      " [40]\n",
      " [12]\n",
      " [ 0]\n",
      " [24]\n",
      " [50]\n",
      " [49]\n",
      " [53]\n",
      " [19]\n",
      " [60]\n",
      " [27]\n",
      " [49]\n",
      " [11]\n",
      " [15]\n",
      " [59]\n",
      " [10]\n",
      " [29]\n",
      " [62]\n",
      " [ 5]\n",
      " [30]\n",
      " [22]\n",
      " [18]\n",
      " [56]\n",
      " [31]\n",
      " [62]\n",
      " [51]\n",
      " [39]\n",
      " [31]\n",
      " [52]\n",
      " [45]\n",
      " [40]\n",
      " [11]\n",
      " [ 1]\n",
      " [48]\n",
      " [42]\n",
      " [58]\n",
      " [45]\n",
      " [37]\n",
      " [62]\n",
      " [59]\n",
      " [38]\n",
      " [ 3]\n",
      " [28]\n",
      " [38]\n",
      " [14]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [13]\n",
      " [38]\n",
      " [36]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [42]\n",
      " [13]\n",
      " [60]\n",
      " [60]\n",
      " [32]\n",
      " [41]\n",
      " [24]\n",
      " [ 7]\n",
      " [19]\n",
      " [24]\n",
      " [25]\n",
      " [33]\n",
      " [11]\n",
      " [ 8]\n",
      " [29]\n",
      " [20]\n",
      " [19]\n",
      " [21]\n",
      " [22]\n",
      " [47]\n",
      " [16]\n",
      " [42]\n",
      " [ 8]\n",
      " [48]\n",
      " [ 6]\n",
      " [12]\n",
      " [46]\n",
      " [58]\n",
      " [ 9]\n",
      " [40]\n",
      " [41]\n",
      " [34]\n",
      " [51]\n",
      " [59]\n",
      " [43]\n",
      " [60]\n",
      " [61]\n",
      " [27]\n",
      " [ 6]\n",
      " [ 1]], shape=(100, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[25 24 16 18 32  8 40 51 40 12  0 24 50 49 53 19 60 27 49 11 15 59 10 29\n",
      " 62  5 30 22 18 56 31 62 51 39 31 52 45 40 11  1 48 42 58 45 37 62 59 38\n",
      "  3 28 38 14 10  7  4 13 38 36  3  6 42 13 60 60 32 41 24  7 19 24 25 33\n",
      " 11  8 29 20 19 21 22 47 16 42  8 48  6 12 46 58  9 40 41 34 51 59 43 60\n",
      " 61 27  6  1], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# random sampling\n",
    "# greedy 贪心策略  random 随机策略\n",
    "sample_indices = tf.random.categorical(logits = example_batch_predictions[0],num_samples=1) # 随机采样\n",
    "# logits 未经过激活函数的输出\n",
    "print(sample_indices)\n",
    "# (100, 65) -> (100, 1)\n",
    "sample_indices = tf.squeeze(sample_indices,axis=-1) # 降维\n",
    "# (100, 1) -> (100,)\n",
    "print(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \" ground, farewell; sweet soil, adieu;\\nMy mother, and my nurse, that bears me yet!\\nWhere'er I wander,\"\n",
      "\n",
      "Output: \"ground, farewell; sweet soil, adieu;\\nMy mother, and my nurse, that bears me yet!\\nWhere'er I wander, \"\n",
      "\n",
      "Predictions:  \"MLDFT.bmb?\\nLlkoGvOk;Cu:Qx'RJFrSxmaSngb; jdtgYxuZ$PZB:-&AZX$,dAvvTcL-GLMU;.QHGIJiDd.j,?ht3bcVmuevwO, \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Output:\",repr(\"\".join(idx2char[target_example_batch[0]])))\n",
    "print()\n",
    "print(\"Predictions: \", repr(\"\".join(idx2char[sample_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "4.1804905\n"
     ]
    }
   ],
   "source": [
    "def loss(labels,logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)\n",
    "\n",
    "model.compile(loss = loss, optimizer='adam')\n",
    "example_loss = loss(target_example_batch,example_batch_predictions) # labels,logits\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 223s 2s/step - loss: 1.6568\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 218s 2s/step - loss: 1.5967\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 210s 2s/step - loss: 1.5496\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 227s 2s/step - loss: 1.5129\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 242s 2s/step - loss: 1.4814\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 248s 2s/step - loss: 1.4568\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 220s 2s/step - loss: 1.4336\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 206s 2s/step - loss: 1.4130\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 233s 2s/step - loss: 1.3944\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 251s 2s/step - loss: 1.3777\n"
     ]
    }
   ],
   "source": [
    "output_dir = './text_generation_checkpoints'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "checkpoint_prefix = os.path.join(output_dir,'ckpt_{epoch}')\n",
    "# model checkpoint save_weights_only\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix,save_weights_only=True)\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(seq_dataset,epochs=epochs,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text_generation_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从文件中加载model （获取最新模型）\n",
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (1, None, 1024)           1311744   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 1,395,009\n",
      "Trainable params: 1,395,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 加载模型 预测\n",
    "model2 = build_model(vocab_size,embedding_dim,rnn_units,batch_size=1)#构建模型, batch_size=1 每次预测一句\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir)) #加载weights\n",
    "model2.build(tf.TensorShape([1,None])) #重新设置输入shape  batch_size=1  \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: know,\n",
      "Why doot Syof:\n",
      "For, scatter than draw they marries wherein's like\n",
      "I think they being content to passe for\n",
      "I should make, that stand the necession with theee what I have not deserted your bod welcome your speedses that you thing. My early good lord; and that stain's rumper'd\n",
      "Bohe called me quite,' and I am at the world grocial couss!\n",
      "That lay a men and nature to her scorn'd, with the world, it these the ale\n",
      "Shall I myself commend the power attend this charityman!\n",
      "I can say agait an exseaters sex' art\n",
      "cold time dase worshipe!\n",
      "\n",
      "Cbudderance! all all this smate the oriole?\n",
      "\n",
      "LADY PARNENLO:\n",
      "Go lover whet resign unto thee to me;\n",
      "But none as the purped this partly sage takes\n",
      "That do I speak no anight?\n",
      "Is not enter to the furst were me with lives.\n",
      "\n",
      "GLOUCESTER:\n",
      "You may do bless'd queen!\n",
      "I pript they means the light.\n",
      "\n",
      "YORK:\n",
      "Who, pifed madd me tword cousin.\n",
      "\n",
      "ARCHUSS long for stand you shall deviser.\n",
      "\n",
      "PAULINA:\n",
      "I do trait a bear, along I less it not.\n",
      "\n",
      "CAMILLO:\n",
      "Bying rust,\n",
      "Authold me thy beholdn\n"
     ]
    }
   ],
   "source": [
    "# 文本生成流程\n",
    "# start ch sequence A,\n",
    "# A -> model -> b\n",
    "# A.append(b) -> B\n",
    "# B(Ab) -> model -> c\n",
    "# B.append(c) -> C\n",
    "# C(Abc) -> model -> ...\n",
    "\n",
    "def generate_text(model, start_string, num_generate = 1000):\n",
    "    input_eval = [char2idx[ch] for ch in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        # 1. model inference -> predictions\n",
    "        # 2. sample -> ch -> text_generated.\n",
    "        # 3. update input_eval\n",
    "        \n",
    "        # predictions : [batch_size, input_eval_len, vocab_size]\n",
    "        predictions = model(input_eval)\n",
    "        # predictions : [input_eval_len, vocab_size]\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # predicted_ids: [input_eval_len, 1]\n",
    "        # a b c -> b c d\n",
    "        predicted_id = tf.random.categorical( predictions, num_samples = 1)[-1, 0].numpy() # 取最后一个\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        # s, x -> rnn -> s', y\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "new_text = generate_text(model2, \"All: \")\n",
    "print(new_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
